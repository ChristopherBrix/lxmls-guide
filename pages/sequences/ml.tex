So far we have not committed to any form for the probability
distributions $\pi_l$, $a_{m,l}$ and $b_l(\obs_i)$. In both applications
addressed in this class, both the observations and the hidden
variables are discrete. The most common approach is to model each of
these probability distributions as multinomial distributions,
summarized in Table \ref{tt:mult-params}. Note that the number of parameters of $a_{l,m}$ is $|\hvocab|(|\hvocab|+1)$ because of the special ``STOP'' symbol.

\begin{table}
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
short notation & probability distribution  & |parameters|& constraint \\
\hline
$\pi_j$ & $p_{\theta} (\hs_1 = \hv_j)$ & $|\hvocab|$ & $\sum_{\hv \in \hvocab} \pi_j = 1$;\\
\hline
$a_{l,m}$ & $p_{\theta} (\hs_i = \hv_l \mid \hs_{i-1} = \hv_m)$ & $|\hvocab|(|\hvocab|+1)$ &$\sum_{\hv_l \in \hvocab} a_{m,l} = 1$;\\
\hline
%$f_{l,m}$ & $p_{\theta} (\hs_N = \hv_l \mid \hs_{N-1} = \hv_m)$ & $(|\hvocab|-1)^2$ &$\sum_{\hv_l \in \hvocab} t_{m,l} = 1$;\\
%\hline
$b_q(l) $& $p_{\theta}(\obs_i = \vv_q\mid \hs_i = \hv_l)$  & $|\hvocab||\vocab|$ &$\sum_{\vv_q \in \vocab} b_q(l)  = 1$.\\
\hline
\end{tabular}
\end{center}
\caption[HMM multinomial parametrization]{\label{tt:mult-params}Multinomial parametrization of the HMM distributions.}
\end{table}

We will refer to the set of all parameters as $\theta$. The HMM model
is trained to maximize the Log Likelihood of the data. Given a
dataset $\mathcal{D}$ the objective being optimized is:
\begin{equation}
\argmax_{\theta} \sum_{\trex} \log \joint
\end{equation}


Multinomial distributions are attractive for several reasons: first of
all, they are easy to implement; secondly, the maximum likelihood estimation of the parameters has a simple closed form. The parameters are just
normalized counts of events that occur in the corpus (the same as the
Na\"{i}ve Bayes from previous class).

Given a labeled corpus, the estimation process consists of counting how
many times each event occurs in the corpus and normalize the counts to
form proper probability distributions. Let us define the following
quantities, called sufficient statistics, that represent the counts of
each event in the corpus:

\begin{align}
\mathbf{Initial \ Counts\!:}\;\;\;\;  &  ic(\hv_l) = \sum_{\trex} 
\Ind (\hs_1 = \hv_j); \label{eq::initialCounts}\\
%
%\mathbf{Final \ Counts\!:}\;\;\;\;  &  fc(\hv_l,\hv _m) = \sum_{\trex} 
%\Ind (\hs_N = \hv_l \mid \hs_{N-1} = \hv_m); \label{eq::finalCounts}\\
%
\mathbf{Transition \ Counts\!:}\;\;\;\;  &  tc(\hv_l,\hv _m) =
\sum_{\trex} \sum_{i = 1}^{N} \Ind (\hs_i = \hv_l \mid \hs_{i-1} = \hv_m); \label{eq::transitionCounts}\\
%
\mathbf{State \ Counts\!:}\;\;\;\;  &  sc(\vv_q,\hv_l) = \sum_{\trex} \sum_{i = 1}^{N}
\Ind (\obs_i = \vv_q\mid \hs_i = \hv_l) \label{eq::stateCounts}
\end{align}

Note that $\Ind$ is an indicator function that has the value 1 when the
particular event happens, and zero otherwise. In words, the previous
equations amount to going through the training corpus and counting how
often each even occurs. For example, eq. \eqref{eq::transitionCounts} counts how often state $\hv_l$ follows state $\hv_m$. Therefore, $tc(``s'',``r'')$ contains the number of times that a sunny day (``s'') followed a rainy day (``r'').

%
%: e.g. the word ``w'' appears with state
%``s'', or state ``s'' follows another state ``s'', or state ``s''
%begins the sentence.


After computing the counts, one can perform some sanity checks
to make sure the implementation is correct. Summing over all entries
of each count table we should observe the following:

\begin{itemize}
\item \textbf{Initial \ Counts\!:} - Should sum to the number of
  sentences.
%\item \textbf{Final \ Counts\!:} - Should sum to the number of sentences.
\item \textbf{Transition \ Counts\!:} - Should sum to the number of
  tokens.
%   minus 2 times the number of sentences. Note that there are
%  N-1 edges for each sentence, and the last edge is being accounted by
%  the final transitions. So this leaves us with N-2 edges per sentence,
%  where N is the number of tokens in that sentence.
\item \textbf{Observation \ Counts\!:} - Should sum to the number of tokens.
\end{itemize}


\begin{exercise}
Convince yourself that the sanity checks described above are true. Collect the counts from a supervised corpus using method
\emph{collect\_counts\_from\_corpus} and use the provided function \emph{sanity\_check\_counts} to perform these checks on the counts table. 
%\begin{python}
% def sanity_check_counts(self,seq_list):
%\end{python}

\begin{python}
In []: run sequences/hmm.py
In []: hmm = HMM(simple)
In []: hmm.collect_counts_from_corpus(simple.train)
In []: hmm.sanity_check_counts(simple.train)
Init Counts match
Final Counts match
Transition Counts match
Observations Counts match
\end{python}
\end{exercise}


Using the sufficient statistics (counts) the parameter estimates are: 
\begin{align}
  \hat{\pi}_l &=  \frac{ic(\hv_l)}{\sum_{\hv_m \in \hvocab}
    ic(\hv_m)}\\
%  \hat{f}_{l,m} &= \frac{fc(\hv_l,\hv_m)}{\sum_{\hv_m \in \hvocab} fc(\hv_l,\hv_m)} \\
  \hat{a}_{l,m} &= \frac{tc(\hv_l,\hv_m)}{\sum_{\hv_l \in \hvocab} tc(\hv_l,\hv_m)} \\
  \hat{b}_l(\vv_q = o) &= \frac{sc(\vv_q,\hv_l)}{\sum_{\vv_p \in\vocab} sc(\vv_p,\hv_l)}
\end{align}


\begin{exercise}
The provided function \emph{train\_supervised} from the \emph{hmm.py} file implements the above parameter estimates.
%Implement a function that estimates the maximum likelihood
%estimates for the parameters given the corpus in the class HMM.
%The function header is in the hmm.py file. 
%\begin{python}
%def train_supervised(self,sequence_list):
%\end{python}
Run this function given the simple dataset above and look at the estimated probabilities. Are they correct? You can also check the variables ending in \emph{\_counts} instead of \emph{\_probs} to see the raw counts (for example, typing \emph{hmm.init\_probs} will show you the raw counts of initial states). How are the counts related to the probabilities?
\begin{python}
In[]:  run sequences/hmm.py
In[]: hmm = HMM(simple)
In[]: hmm.train_supervised(simple.train)
In []: hmm.init_probs
Out[]: 
array([[ 0.66666667],
       [ 0.33333333]])
In []: hmm.transition_probs
Out[]: 
array([[ 0.5  ,  0.   ],
       [ 0.5  ,  0.625],
       [ 0.   ,  0.375]])
In []: hmm.observation_probs
Out[]: 
array([[ 0.75 ,  0.25 ],
       [ 0.25 ,  0.375],
       [ 0.   ,  0.375],
       [ 0.   ,  0.   ]])
\end{python}
\end{exercise}


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../../guide"
%%% End: 
