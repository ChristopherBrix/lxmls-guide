In this lab, we will work with Amazon.com's Web Services\footnote{http://aws.amazon.com/}, a cloud based solution
to run some simple analyses. Then, in the next lab, we will build on these
tools to construct a larger learning system.

\section{Introduction}

AWS is a set of services of different types. We will focus on the \emph{Elastic
Compute Cluster}, EC2.\footnote{http://aws.amazon.com/ec2/}

In this lab, we will be logging in to Linux-based machines using a command line
interface. If you are not familiar with the command line, don't worry -- we will tell you everything you need regarding the command line, and your exercises only involve Python, just like the previous days.

\section{Useful Information}

You should have received a piece of paper with the following four pieces of
information:

\begin{enumerate}
\item A host name
\item A password
\item A ``AWS Key''
\item A ``AWS Secret''
\end{enumerate}

We will ignore the last two items for the moment.  Your hostname will be
something like \texttt{ec2-54-234-117-69.compute-1.amazonaws.com} and you
should be able to log in with username \texttt{ubuntu} and the password you
were given! This is your personal server.

If you are using the command line, you can just type:

ssh ubuntu@ec2-54-234-117-69.compute-1.amazonaws.com

(accept the RSA key and log in with the password we provided you).

The machine has all of the necessary Python packages for you to work. It also
has a standard collection of editors and other tools. It is an Ubuntu based
installation and you can access super user powers with \texttt{sudo} (if you're not familiar with Unix-based environments, you might not understand this last sentence -- don't worry about it for now).


\section{MapReduce \& Hadoop}

The initial idea of MapReduce was an implementation by google\footnote{http://research.google.com/archive/mapreduce.html}, but very soon
Hadoop appeared as an open-source implementation of the same paradigm.\footnote{http://hadoop.apache.org/}

The basic idea is to take your original problem, whichever it may be, and tackle it in two steps:
%
\begin{enumerate}
\item \textbf{Map step:} Divide the data into several parts and send each part to a different computer. Each computer does some computation using \emph{only} that part of the data, and returns some output.\footnote{The name "map" is completely unrelated to \emph{maximum a posteriori} (MAP) inference which was introduced in day 1.}
\item \textbf{Reduce step:} Collect all the outputs from all the different computers and compute the final solution from those outputs.
\end{enumerate}

\subsection{Example: Word Count}

The classic example of application of MapReduce is the word count problem:

\begin{itemize}
\item You have a collection of documents, there are many of them.
\item You want to count how many times each word appears in the whole collection.
\end{itemize}

If the text corpus is small, this is trivially easy to do on any computer. However, for big corpora, one computer alone would take a long time. For example, the whole English Wikipedia is about 10 GB compressed, and several times that when uncompressed. It would take a considerable time to count the occurrence of each word on the whole dataset using only one computer.

However, this problem is quite easy to parallelize using the MapReduce framework:

\begin{enumerate}
\item You process each document by itself, counting how many times each word
appears. This is the \emph{map} step. Notice that each document can be
processed in parallel by different machines. The result for each file is a
dictionary of a count for each word.
\item You sum up the counts for each word to get the final count. This is the
\emph{reduce} step. Notice that you can now parallelize over words.
\end{enumerate}

In this tutorial, we will be using the package \texttt{MrJob}\footnote{MrJob was developed by Yelp, the company behind the epinomious website; it is available
as open source, under the Apache license -- see http://pythonhosted.org/mrjob/}. On your Amazon machine, the package
is already installed with all its dependencies, so you can just go ahead and
use it.

We are going to implement the word count algorithm now:

\begin{python}
# Import the necessary libraries:
from mrjob.job import MRJob
import re

# Check whether the word is a real word
def _is_valid_word(w):
    return re.search(r'\W', w) is None

class WordCount(MRJob):
    def mapper(self, _, doc):
        c = {}
        # Process the document
        for w in doc.split():
            if _is_valid_word(w):
                if w in c:
                    c[w] += 1
                else:
                    c[w] = 1

        # Now, output the results
        for w,c in c.items():
            yield w,c

    def reducer(self, key, cs):
        yield key, sum(cs)

wc = WordCount()
wc.run()
\end{python}

You can save this as \texttt{wordcount.py} and run this on your machine
directly. We will also make use of the \texttt{/datasets} directory that is
preinstalled on your machine:

\begin{verbatim}
python wordcount.py /datasets/small-text-file.txt
\end{verbatim}

\subsubsection{Running Word Count using Hadoop}

We are now going to use the AWS Key and AWS Secret. These are like a username
and password for Amazon Webservices.

One of the advantages of MrJob is that you can use the exact same Python code as before, but changing a few arguments in the command line will run the Word Count on Amazon's computers instead of on the computer where the file is. This is what Amazon calls Elastic Map Reduce (EMR).

To run the exact same code as before using EMR, type the following into the command line.

\begin{verbatim}
export AWS_ACCESS_KEY_ID=<YOUR ACCESS KEY>
export AWS_SECRET_ACCESS_KEY=<YOUR SECRET KEY>

python wordcount.py \
        -r emr \
        /datasets/small-text-file.txt /
        --num-ec2-instances 2 \
        --aws-region eu-west-1
\end{verbatim}

This runs the code on Elastic Map Reduce with 2 instances.

\subsubsection{Running Word Count on Wikipedia}

This file is not very big, but running on a cluster of machine allows us to
scale up run on Wikipedia

\begin{verbatim}
python wordcount.py \
        -r emr \
        s3://lxmls-labs/pt_perline10.txt
        --num-ec2-instances 10 \
        --aws-region eu-west-1
\end{verbatim}

This should take about 3 minutes.

\subsection{Using Naïve Bayes for Language Detection}

The implementation of binary Naïve Bayes is very similar to counting words: we just
need to keep track of the class of each document and maintain.

\begin{python}
# Import the necessary libraries:
from mrjob.job import MRJob
import re

# Check whether the word is a real word
def _is_valid_word(w):
    return re.search(r'\W', w) is None

class TrimerCount(MRJob):
    def mapper(self, _, doc):
        c = {}
        # Process the document
        for i in xrange(len(doc)-3):
            w = doc[i:i+3]
            if _is_valid_word(w):
                if w in c:
                    c[w] += 1
                else:
                    c[w] = 1

        # Now, output the results
        for w,c in c.items():
            yield w,c

    def reducer(self, key, cs):
        yield key, sum(cs)

TrimerCount.run()
\end{python}

We will run two jobs, one on a selection of the Portuguese Wikipedia (10\%) and
another on the English Wikipedia (only 1\% as this is much larger).

\begin{verbatim}
python trimercount.py \
        -r emr \
        s3://lxmls-labs/pt_perline10.txt
        --num-ec2-instances 10 \
        --aws-region eu-west-1 > pt.counts.txt

python trimercount.py \
        -r emr \
        s3://lxmls-labs/en_perline01.txt
        --num-ec2-instances 10 \
        --aws-region eu-west-1 > en.counts.txt
\end{verbatim}

These should run in a few minutes as well. Running it on the whole Wikipedia
could be done with the same system in just a few hours.

\subsection{Building a Classifier}

We need to load the outputs from the Hadoop runs:

\begin{python}
def load_counts(ifile):
    counts = {}
    with open(ifile) as input:
        for line in input:
            word, count = line.strip().split()
            word = word[1:-1]
            counts[word] = float(count)
    return counts
\end{python}

We write a function for the classification:

\begin{python}
def score(counts_pt, counts_en, test):
    val = 1.
    for i in xrange(len(test)-3):
        tri = test[i:i+3]
        tri_pt = counts_pt.get(tri, 1.)
        tri_en = counts_en.get(tri, 1.)
        val *= tri_pt/tr_en
    return val
\end{python}

We now load the two files and then classify any sentences the user types:

\begin{python}
counts_pt = load_counts('output.pt.txt')
counts_en = load_counts('output.en.txt')

while True:
    test = raw_input("Type a test sentence? ")
    if not test: break
    print score(counts_pt, counts_en, test)
\end{python}


