In this lab, we will work with Amazon.com's Web Services\footnote{http://aws.amazon.com/}, a cloud based solution
to run some simple analyses. Then, in the next lab, we will build on these
tools to construct a larger learning system.

\section*{If you use your own computer}

The instructions we will provide here assume that you are using one of our own computers, which have some software preinstalled. Unfortunately, EC2 installation guides and tutorials are not very widespread. If you are using your own machine, please follow the installation instructions for the EC2 tools available at

http://aws.amazon.com/developertools/351

in the "Download" section.

\section*{Today's Assignment}

Today we will show you how to solve a simple problem (counting words in documents) in a distributed manner. We will then ask you to implement a distributed solution for a problem you already know (Na\"{i}ve Bayes classification).

\section{Introduction}

AWS is a set of services of different types. We will focus on the \emph{Elastic
Compute Cluster}, EC2.\footnote{http://aws.amazon.com/ec2/}

In this lab, we will be logging in to Linux-based machines using a command line
interface. If you are not familiar with the command line, don't worry -- we will tell you everything you need regarding the command line, and your exercises only involve Python, just like the previous days.

\section{Useful Information}

You should have received a piece of paper with the following four pieces of
information:

\begin{enumerate}
\item A host name
\item A password
\item A ``AWS Key''
\item A ``AWS Secret''
\end{enumerate}

We will ignore the last two items for the moment.  Your hostname will be
something like \texttt{ec2-54-234-117-69.compute-1.amazonaws.com} and you
should be able to log in with username \texttt{ubuntu} and the password you
were given! This is your personal server.

If you are using the command line, you can just type:

ssh ubuntu@ec2-54-234-117-69.compute-1.amazonaws.com

(accept the RSA key and log in with the password we provided you).

The machine has all of the necessary Python packages for you to work. It also
has a standard collection of editors and other tools. It is an Ubuntu based
installation and you can access super user powers with \texttt{sudo} (if you're not familiar with Unix-based environments, you might not understand this last sentence -- don't worry about it for now).

\section{MapReduce for Word Count}



The initial idea of MapReduce was an implementation by google\footnote{http://research.google.com/archive/mapreduce.html}, but very soon
Hadoop appeared as an open-source implementation of the same paradigm.\footnote{http://hadoop.apache.org/}

The basic idea is to take your original problem, whichever it may be, and tackle it in two steps:
%
\begin{enumerate}
\item \textbf{Map step:} Divide the data into several parts and send each part to a different computer. Each computer does some computation using \emph{only} that part of the data, and returns some output.\footnote{The name "map" is completely unrelated to \emph{maximum a posteriori} (MAP) inference which was introduced in day 1.}
\item \textbf{Reduce step:} Collect all the outputs from all the different computers and compute the final solution from those outputs.
\end{enumerate}

The classic example of application of MapReduce is the word count problem:

\begin{itemize}
\item You have a collection of documents, there are many of them.
\item You want to count how many times each word appears in the whole collection.
\end{itemize}

If the text corpus is small, this is trivially easy to do on any computer. However, for big corpora, one computer alone would take a long time. For example, the whole English Wikipedia is about 10 GB compressed, and several times that when uncompressed. It would take a considerable time to count the occurrence of each word on the whole dataset using only one computer.

However, this problem is quite easy to parallelize using the MapReduce framework:

\begin{enumerate}
\item You process each document by itself, counting how many times each word
appears. This is the \emph{map} step. Notice that each document can be
processed in parallel by different machines. The result for each file is a
dictionary of a count for each word.
\item You sum up the counts for each word to get the final count. This is the
\emph{reduce} step. Notice that you can now parallelize over words.
\end{enumerate}

\subsection{Running Word Count locally}

In this tutorial, we will be using the package \texttt{MrJob}\footnote{MrJob was developed by Yelp, the company behind the epinomious website; it is available
as open source, under the Apache license -- see http://pythonhosted.org/mrjob/}. On your Amazon machine, the package
is already installed with all its dependencies, so you can just go ahead and
use it.

We are going to implement the word count algorithm now:

\begin{python}
# Import the necessary libraries:
from mrjob.job import MRJob

class WordCount(MRJob):
    def mapper(self, _, doc):
        c = {}
        # Process the document
        for w in doc.split():
            if w in c:
                c[w] += 1
            else:
                c[w] = 1

        # Now, output the results
        for w,c in c.items():
            yield w,c

    def reducer(self, key, cs):
        yield key, sum(cs)

wc = WordCount()
wc.run()
\end{python}

This file already exists as \texttt{wordcount.py} (inside the \texttt{big\_data} folder). To run it on your machine (without using Amazon EC2), navigate to the \texttt{labs} folder\todo{No final temos de ver se e esta a pasta.} and type this into a terminal:

\begin{verbatim}
python wordcount.py ../data/wikipedia/en_perline001.txt > results.txt
\end{verbatim}

Open the \texttt{results.txt} file in your favorite text editor. Each line of this file contains a word and the corresponding count.

\subsection{Running Word Count on Amazon EC2}

We are now going to use the AWS Key and AWS Secret. These are like a username
and password for Amazon Webservices.

One of the advantages of MrJob is that you can use the exact same Python code as before, but changing a few arguments in the command line will run the Word Count on Amazon's computers instead of on the computer where the file is. This is what Amazon calls Elastic Map Reduce (EMR).

To run the exact same code as before using EMR, type the following into the command line.

\begin{verbatim}
export AWS_ACCESS_KEY_ID=<YOUR ACCESS KEY>
export AWS_SECRET_ACCESS_KEY=<YOUR SECRET KEY>

python wordcount.py \
        -r emr \
        /datasets/small-text-file.txt /
        --num-ec2-instances 2 \
        --aws-region eu-west-1
\end{verbatim}

This runs the code on Elastic Map Reduce with 2 instances.

The file we used on the previous exercises is not very big, but running on a cluster of machines allows us to
scale up enough to use Wikipedia as input:

\begin{verbatim}
python wordcount.py \
        -r emr \
        s3://lxmls-labs/pt_perline10.txt
        --num-ec2-instances 10 \
        --aws-region eu-west-1
\end{verbatim}

This should take about 3 minutes.

\section{Using Na\"{i}ve Bayes for Language Detection}

Language detection is surprisingly easy if you have enough data to train your system. In our case, we're going to use triplets of characters (or "trimers") as features. For example, if your whole training corpus is a sentence in English which reads "I love Lisbon" (length: 13 characters) and a sentence in Portuguese which reads "Adoro Lisboa" (length: 12 characters), you would say that you saw the following features: "I l", " lo", "lov", "ove", "ve ", and so on in the English data, and "Ado", "dor", "oro", "ro ", and so on in the Portuguese data. Note the presence of whitespace on some of these trimers.

You can now use the exact same Na\"{i}ve Bayes algorithm which you used for sentiment analysis on Day 1. Instead of classifying text into two classes (positive and negative) we're going to classify text into two other classes (Portuguese and English). Our training data will be parts of the Portuguese and English Wikipedias.\footnote{We will use 10\% of the Portuguese Wikipedia and 1\% of the English Wikipedia to ensure that you can complete this exercise in the time of this lab session. However, Amazon's infrastructure is easily capable of handling the whole Wikipedia.}

The implementation of binary Na\"{i}ve Bayes is very similar to counting words:
\begin{enumerate}
	\item The map step takes the whole training data of the Portuguese language and splits it into documents. The mapper function computes the frequency of each trimer on one document.
	\item The reduce step compiles the information from all the documents and sums the counts of every Portuguese document.
	\item The previous two steps are repeated for the English training data.
	\item After the MapReduce part, a post-processing step uses these counts with similar formulas as in Day 1 to classify new unseen text into the two classes: English or Portuguese. For convenience, we repeat these formulas below.
\end{enumerate}

Once you have the counts of trimer occurrences on each language, you need to estimate:
\begin{itemize}
	\item The \emph{prior} probability of each language appearing at test time, ${\hat P}(\text{PT})$ and ${\hat P}(\text{EN})$. For simplicity, instead of using Maximum Likelihood estimation, let's assume that the users of your language detector are equally likely to try English and Portuguese sentences. Thus, ${\hat P}(\text{PT}) = {\hat P}(\text{EN}) = \frac{1}{2}$.\footnote{You could also assume, as in Day 1, that the probability of a given language appearing at test time is proportional to the size of the training data of that language. Each assumption makes sense in different contexts.}
	\item The probability of each feature (trimer) given the class, ${\hat P}(t_j | c)$. We will again use Maximum Likelihood estimation, which means that the probability of trimer $t$ given language $c$ is equal to the number of times $t$ occurred in documents of language $c$, divided by the total number of trimers in language $c$. Mathematically:
\begin{equation}
{\hat P}(t_j|\text{PT}) = \frac{n(t_j,\text{PT})}{\sum_j n(t_j,\text{PT})},
\end{equation}
where $n(t_j,\text{PT})$ is the number of times the trimer $t_j$ occurred in your Portuguese training corpus. A similar formula is used for ${\hat P}(t_j|\text{EN})$.
\end{itemize}

Then, given a test sentence $x$, we can compute the following argmax for the two classes $c = \text{PT}$ and $c = \text{EN}$:
%
\begin{eqnarray}
\argmax_c {\hat P}(c | x) &= \argmax_c {\hat P}(c) {\hat P}(x | c) \nonumber\\
&= \argmax_c {\hat P}(c) \prod_j {\hat P}(c | t_j) \nonumber\\
&= \argmax_c \prod_j {\hat P}(c | t_j) \nonumber\\
&= \argmax_c \sum_j \log({\hat P}(c | t_j))\label{eq:NBformula}
\end{eqnarray}
%
In the first equality we used Bayes' Theorem. In the second one we used the assumption of conditional independence of features given the class, which is at the core of Na\"{i}ve Bayes. In the third one, we used that ${\hat P}(\text{PT}) = {\hat P}(\text{EN}) = \frac{1}{2}$, thus the prior does not affect the argmax. In the last equation, we used the fact that the argmax is not affected by the application of a logarithm. Logarithms will prevent your program from encountering underflow errors when multiplying many numbers which are very small.

\section{Assignment}

Using the Word Count example we've given you above, implement the Na\"{i}ve Bayes language detector described above. You should do this in two parts:

\begin{itemize}
	\item Steps 1 to 3 (counting occurrences of trimers in train data, for both languages), should be run on EC2. If you run the script named ??, these counts will run on EC2 and their output to files named ?? and ??. However, there is some code missing in file ?? which you must complete.
	\item Step 4 should be run locally (it is quite fast even with just one computer). It should use the two files mentioned in the previous point and implement the formula given in equation \eqref{eq:NBformula}.
\end{itemize}

%\begin{python}
%# Import the necessary libraries:
%from mrjob.job import MRJob
%import re
%
%class TrimerCount(MRJob):
%    def mapper(self, _, doc):
%        c = {}
%        # Process the document
%        for i in xrange(len(doc)-3):
%            w = doc[i:i+3]
%            if w in c:
%                c[w] += 1
%            else:
%                c[w] = 1
%
%        # Now, output the results
%        for w,c in c.items():
%            yield w,c
%
%    def reducer(self, key, cs):
%        yield key, sum(cs)
%
%TrimerCount.run()
%\end{python}

We will run two jobs, one on a selection of the Portuguese Wikipedia (10\%) and
another on the English Wikipedia (only 1\% as this is much larger).

\begin{verbatim}
python trimercount.py \
        -r emr \
        s3://lxmls-labs/pt_perline10.txt
        --num-ec2-instances 10 \
        --aws-region eu-west-1 > pt.counts.txt

python trimercount.py \
        -r emr \
        s3://lxmls-labs/en_perline01.txt
        --num-ec2-instances 10 \
        --aws-region eu-west-1 > en.counts.txt
\end{verbatim}

These should run in a few minutes as well. Running it on the whole Wikipedia
could be done with the same system in just a few hours.

%\subsection{Building a Classifier}
%
%We need to load the outputs from the Hadoop runs:
%
%\begin{python}
%def load_counts(ifile):
%    counts = {}
%    with open(ifile) as input:
%        for line in input:
%            word, count = line.strip().split()
%            word = word[1:-1]
%            counts[word] = float(count)
%    return counts
%\end{python}
%
%We write a function for the classification:
%
%\begin{python}
%def score(counts_pt, counts_en, test):
%    val = 1.
%    for i in xrange(len(test)-3):
%        tri = test[i:i+3]
%        tri_pt = counts_pt.get(tri, 1.)
%        tri_en = counts_en.get(tri, 1.)
%        val *= tri_pt/tr_en
%    return val
%\end{python}
%
%We now load the two files and then classify any sentences the user types:
%
%\begin{python}
%counts_pt = load_counts('output.pt.txt')
%counts_en = load_counts('output.en.txt')
%
%while True:
%    test = raw_input("Type a test sentence? ")
%    if not test: break
%    print score(counts_pt, counts_en, test)
%\end{python}

